# Model Configuration
model:
  name: mae_vit_base_patch16
  pretrained_path: "./pretrained_models/mae_pretrain_vit_base.pth" # Path to pre-trained model weights
  patch_size: 8
  mask_ratio: 0.75

# Loss Configuration
loss:
  name: betti_matching
  norm_pix_loss: true
  alpha: 0.005
  alpha_warmup_epochs: 0
  alpha_mse_treshold: 0 # If mse_loss > treshold, use mse loss purely
  filtration: "superlevel"  # Available options: "superlevel", "sublevel", "bothlevels"
  push_unmatched_to_1_0: false
  barcode_length_threshold: 0.001 # ignore barcodes with length < threshold, set to a small value
  sphere: false
  calculate_channels_separately: false # Calculate Betti numbers for each channel separately
  num_processes: 8
  log_timing: false # Log timing for Betti Matching loss computation
  
# Data Configuration
data:
  input_size: 48
  augmentation:
    name: CenterCrop 
    # max_scale: 1.0 # Maximum scale for RandomResizedCrop
    # min_scale_weight: 0.2 # Minimum scale for RandomResizedCrop
    # max_ratio: 1.33 # Maximum aspect ratio for RandomResizedCrop
  data_path: "./data/pretrain/massgis_split_test/train"
  eval_data_path: "./data/pretrain/massgis_split_test/val"
  num_workers: 8

# Pre-training Hyperparameters
training:
  # Optimizer parameters
  optimizer: adamw
  weight_decay: 0.05
  beta1: 0.9
  beta2: 0.95
  frozen_encoder_epochs: 2 # Number of epochs to freeze encoder, set to 0 for no freezing
  # Learning Rate Schedule
  learning_rate: 1.e-4 # Base LR
  min_lr: 0.0 # Lower bound
  warmup_epochs: 1
  warmup_lr_init: 0.0 # Can use to have high LR for decoder during warmup
  # Training loop
  epochs: 3 # Adjust as needed
  batch_size: 256 # Adjust based on GPU memory
  gradient_accumulation_steps: 1 # Gradient accumulation steps
  # Misc
  mixed_precision: true # Use mixed precision training
  seed: 42

# Logging & Checkpointing
logging:
  enable_wandb: false
  wandb_project: "topo-conv-mae-pretrain" # Your W&B project name
  wandb_run_name: "timing_quick_optimized" # Optional custom run name
  # wandb_run_id: "cnn_mae_bm_test" # Optional custom run id
  output_dir: "./output/pretrain/vitmae_test" # Checkpoint dir
  val_interval: 1 # validation interval
  log_interval: 1 # wandb log interval
  include_vis: false # Include visualizations
  # vis_interval: 1 # visualization interval
  checkpointing: false
