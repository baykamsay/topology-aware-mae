# Model Configuration
model:
  name: cnnmae_small
  patch_size: 4  # For 56x56 input
  # Encoder parameters
  encoder_embed_dim: 64      # Base dimension for CNN encoder's first stage
  encoder_depths: [2, 6]  # Number of blocks per stage (matches mae_cnn_small default)
  # Decoder parameters
  decoder_dim: 256           # Base dimension for CNN decoder (updated from previous version)
  # Other parameters
  use_batchnorm: true        # Use batch normalization
  mask_ratio: 0.75
  norm_pix_loss: true

# Loss Configuration
loss:
  name: mse
  
# Data Configuration
data:
  input_size: 56
  data_path: "./data/pretrain/roads_mini/train"
  eval_data_path: "./data/pretrain/roads_mini/val"
  num_workers: 16

# Pre-training Hyperparameters
training:
  # Optimizer parameters
  optimizer: adamw
  weight_decay: 0.05
  beta1: 0.9
  beta2: 0.95
  # Learning Rate Schedule
  learning_rate: 5.0e-4 # Base LR
  min_lr: 1.0e-5 # Lower bound
  warmup_epochs: 1
  # Training loop
  epochs: 10 # Adjust as needed
  batch_size: 256 # Adjust based on GPU memory
  gradient_accumulation_steps: 1 # Gradient accumulation steps
  # Misc
  mixed_precision: true # Use mixed precision training
  seed: 42

# Logging & Checkpointing
logging:
  enable_wandb: true
  wandb_project: "topo-conv-mae-pretrain" # Your W&B project name
  wandb_run_name: "cnn_mae_base_10" # Optional custom run name
  output_dir: "./output/pretrain/test" # Checkpoint dir
  val_interval: 1 # validation interval
  log_interval: 1 # wandb log interval
  include_vis: true # Include visualizations
  vis_interval: 1 # visualization interval
  checkpointing: false
