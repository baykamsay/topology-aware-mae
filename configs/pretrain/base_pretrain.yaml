# Model Configuration
model:
  name: convnextv2_test
  patch_size: 16  # For 128x128 input
  decoder_depth: 1
  decoder_embed_dim: 512
  mask_ratio: 0.6
  norm_pix_loss: true

# Loss Configuration
loss:
  name: mse
  
# Data Configuration
data:
  input_size: 128
  data_path: "./data/pretrain/roads/train"
  eval_data_path: "./data/pretrain/roads/val"
  num_workers: 8
  pin_mem: true

# Pre-training Hyperparameters
training:
  # Optimizer parameters
  optimizer: adamw
  weight_decay: 0.05
  beta1: 0.9
  beta2: 0.95
  # Learning Rate Schedule
  blr: 1.5e-4 # Base LR
  min_lr: 0.0 # Lower bound
  warmup_epochs: 40
  # Training loop
  epochs: 800 # Adjust as needed
  batch_size: 64 # Adjust based on GPU memory
  update_freq: 1 # Gradient accumulation steps
  seed: 42

# Logging & Checkpointing
logging:
  log_dir: "./output/logs/pretrain" # Tensorboard/Output log dir
  output_dir: "./output/checkpoints/pretrain" # Checkpoint dir
  auto_resume: false
  resume: null
  enable_wandb: true
  wandb_project: "mae_pretrain_roads" # Your W&B project name
  wandb_entity: null # Your W&B username/team (optional)
  run_name: "convnextv2_initial" # Optional custom run name
  print_freq: 20 # Print every N iterations
  vis_freq: 20 # Visualize every N epochs
  save_ckpt_freq: 50 # Save checkpoint every N epochs

# Distributed Training (defaults, often overridden by environment)
distributed:
  dist_url: "env://"
  world_size: 1
  local_rank: -1