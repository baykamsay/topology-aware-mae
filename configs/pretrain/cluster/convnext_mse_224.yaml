# Model Configuration
model:
  name: convnextv2_base
  pretrained_path: "/vol/miltank/users/sayb/projects/topo-conv-mae/pretrained_models/convnextv2_base_1k_224_fcmae.pt" # Path to pre-trained model weights
  patch_size: 32
  decoder_depth: 1
  decoder_embed_dim: 1024
  mask_ratio: 0.6

# Loss Configuration
loss:
  name: mse
  norm_pix_loss: true
  
# Data Configuration
data:
  input_size: 224
  augmentation:
    name: RandomResizedCrop 
    scale: [0.2, 1.0] # Scale range for RandomResizedCrop
    ratio: [0.75, 1.3333] # Aspect ratio range for RandomResizedCrop
  data_path: "/vol/miltank/users/sayb/datasets/roads_filtered_224/train"
  eval_data_path: "/vol/miltank/users/sayb/datasets/roads_filtered_224/val"
  num_workers: 16

# Pre-training Hyperparameters
training:
  # Optimizer parameters
  optimizer: adamw
  weight_decay: 0.05
  beta1: 0.9
  beta2: 0.95
  frozen_encoder_epochs: 10
  # Learning Rate Schedule
  learning_rate: 1.e-4 # Base LR
  min_lr: 0.0 # Lower bound
  warmup_epochs: 20
  # Training loop
  epochs: 200 # Adjust as needed
  batch_size: 96 # Adjust based on GPU memory
  gradient_accumulation_steps: 1 # Gradient accumulation steps
  # Misc
  mixed_precision: true # Use mixed precision training
  seed: 42

# Logging & Checkpointing
logging:
  enable_wandb: true
  wandb_project: "topo-conv-mae-pretrain" # Your W&B project name
  wandb_run_name: "convnext_mse_pretrained_base_2" # Optional custom run name
  wandb_run_id: "convnext_mse_pretrained_base_2" # Optional custom run id
  output_dir: "/vol/miltank/users/sayb/projects/topo-conv-mae/output/pretrain/convnext_mse_pretrained_base_2" # Checkpoint dir
  val_interval: 1 # validation interval
  log_interval: 1 # wandb log interval
  include_vis: true # Include visualizations
  vis_interval: 5 # visualization interval
  checkpointing: true
